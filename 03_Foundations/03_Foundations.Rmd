---
title: "MLM Intuitions, Implementation in R, Random Slopes"
subtitle: "MLM Lab Workshop"
author: "Michael Hallquist"
date: "18Feb2021"
output:
  html_document:
    code_folding: hide
    df_print: kable
    mathjax: default
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_depth: 2
  pdf_document:
    code_folding: hide
    df_print: kable
    number_sections: yes
    toc: yes
    toc_depth: 2
---
<style type="text/css">
body{ font-size: 24px; max-width: 1600px; margin: auto; padding: 1em; }
code.r{ font-size: 20px; }
p { padding-top: 8px; padding-bottom: 8px; }
pre { font-size: 16px; }
</style>

```{r setup, include=FALSE}
if (!require(pacman)) { install.packages("pacman"); library(pacman) }
p_load(knitr, tidyverse, broom, lme4, modelr, performance, afex, emmeans, cowplot, insight, merTools,
       ggeffects, sjPlot)
knitr::opts_chunk$set(echo = TRUE) #print code by default
options(digits=3, width=100) 
```


# Intuition from application: Flanker Task

Let's look at experimental data from a common cognitive task, the flanker task. These are real data recently published in Hall, Schreiber, Allen, & Hallquist, *Journal of Personality*. This is a sample of 107 young adults who completed a version of the Eriksen flanker task.). In each trial, participants saw five horizontal arrows and were told to press a key corresponding to the direction of the center arrow (left or right). Participants were instructed to respond as quickly and accurately as possible. Half of the trials presented arrows on either side of the center arrow that pointed in the same direction ("congruent" trials), whereas the flanking arrows on the other trials pointed in the opposite direction ("incongruent" trials). 

Participants completed 160 trials in two conditions: mostly congruent (70% of trials congruent) and mostly incongruent (70% of trials incongruent). Each condition was split into blocks of 40 trials. The direction of the central arrow was counterbalanced across trials. Four blocks of forty trials were presented in ABBA order, where A is a mostly congruent block and B is a mostly incongruent block. Stimuli were displayed for 1000ms each with a 500ms inter-trial interval (ITI).

Congruent: $\leftarrow \leftarrow \leftarrow \leftarrow \leftarrow$

Incongruent: $\leftarrow \leftarrow \rightarrow \leftarrow \leftarrow$

## Data structure

```{r}
flanker <- readRDS(file="flanker_jop.rds") %>% arrange(id, trial)
str(flanker)
```

The key dependent variables are reaction time (`rt`) and accuracy (`correct`). We will defer models of accuracy for a later workshop. Let's just focus on rt for today.

```{r rtdist, fig=TRUE}
lattice::histogram(~ rt | cond*block, flanker)
```

Notice how non-normal the RTs are? This is a familiar problem (e.g., see the classic Ratcliff 1993 *Psychological Bulletin* paper) and we will spend more time on it in a few weeks. For now, we can assume that the strong positive skew will be a problem when we fit a model (GLM or MLM) that assumes approximately normal residuals.

Therefore, for now, we will transform the RTs to approximate normality for analysis. Two transformations are often effective: logarithm and inverse. Note that for the inverse ($1/x$) transformation, it is useful to take the *negative* inverse so that higher values still represent longer RTs. Furthermore, the inverse often leads to very tiny values, so scaling up by a large constant (100 or 1000) can make the variable more practically useful (avoid tiny regression coefficients, for example).


```{r rtlog, fig=TRUE}
flanker <- flanker %>% mutate(
  rt_log10 = log10(rt),
  rt_inv = -1000/rt
)
  
lattice::histogram(~ rt_log10 | cond*block, flanker)
```

```{r rtinv, fig=TRUE}
lattice::histogram(~ rt_inv | cond*block, flanker)
```

Here, the inverse RT looks awfully good -- we'll use this moving forward.

### Person-period form (long form)

Thinking back to our multilevel structure, we have 160 trials nested within 107 people. Thus, our dataset should be 17,120 rows in long form. (Note that if there were missing or dropped trials, this number could be lower). For virtually all MLMs (in R), we want our data in person-period form (more generically, 'long form') where one variable encodes person (level 2 unit) and another encodes the period or repetition for each observation (level 1 unit). Other variables would encode predictors or outcome variables.

```{r}
head(flanker)
```

### L1 and L2 variables

Let's think through what variables exist at the within-person (L1) and between-person (L2) levels.

*Level 1* -- *within-person*

1. rt: reaction time (in milliseconds)
2. cond: congruent, incongruent (type of flanker display)
3. block: most_incon, most_con (proportion of incongruent or congruent trials in a block)
4. trial: linear trial number
5. run_trial: linear trial number within each run (consisting of 40 trials)

*Level 2* -- *between-person*

1. exclude: An exclusion variable for sensitivity analysis, where higher values denote more dubious data
2. MPS_aggression: The MPQ aggression subscale (personality)
3. MPS_alienation: The MPQ alienation subscale (personality)

*Unit variables*

1. id: The subject ID number
2. run: The run number (4 per person)

## Basic research questions

1. Are RTs slower for incongruent relative to congruent trials?
2. Are RTs slower for mostly incongruent blocks relative to mostly congruent blocks?
3. Are congruent RTs slower in mostly incongruent blocks relative to mostly congruent blocks?
3. Do RTs change substantially with experience (i.e., as trial increases)?

# Fitting things the wrong way: GLM

Recall from Alison's lecture that when we violate the assumption of independent residuals, we probably need to move away from the standard general linear model (GLM). The level of non-independence of observations within a cluster is proportionate to the intraclass correlation:

$$
ICC = \frac{s^2_b}{s^2_b + s^2_w}
$$

What happens if we plow through these considerations and try to address our questions above?

Our basic model of the task would be:

$$
\textrm{RT}_{ij} = \beta_{0} + \beta_{1} \textrm{block}_{ij} + \beta_{2} \textrm{cond}_{ij} +
  \beta_{3}  \textrm{block}_{ij} \cdot \textrm{cond}_{ij} + \varepsilon_{ij} \\
\boldsymbol{\varepsilon} \sim \mathcal{N}(0, \sigma^2)
$$
where we have trial $i$ nested within subject $j$.

**Note**: none of the coefficients varies by subject or trial, only the residuals do!

```{r, fig=TRUE}
lm_fit <- lm(rt_inv ~ block*cond, flanker)
summary(lm_fit)
car::Anova(lm_fit, type=3)
summary(emblock <- emmeans(lm_fit, ~block))
pairs(emblock)

summary(emcond <- emmeans(lm_fit, ~cond))
pairs(emcond)
```

## What if we want these estimates on the original (back-transformed scale?)

Force back onto original RT using custom function

```{r}
rg <- update(ref_grid(lm_fit), tran=list(linkinv=function(x) { -1000/x }, mu.eta=function(x) { -1000/x }), predict.type="response")
summary(res_trans <- emmeans(rg, ~cond|block))
summary(emmeans(rg, ~cond))
summary(emmeans(rg, ~block))

emmip(rg, ~cond|block)

pairs(emmeans(lm_fit, ~block|cond))

ggplot(lm_fit, aes(sample = rstandard(lm_fit))) + 
  geom_qq() + stat_qq_line() + 
  ggtitle("Normal Q-Q plot on standardized residuals")

```


At a high level, we might conclude that there is some evidence that:

1. RTs are about 20ms slower for incongruent relative to congruent trials
2. RTs are somewhat *faster* in mostly incongruent blocks
3. Congruent RTs are slightly faster in mostly incongruent blocks
4. Incongruent trials are much slower in mostly congruent compared to mostly congruent blocks!

## We knows it's wrong (conceptually), but can we see the violations?

```{r}
df <- flanker %>% add_residuals(model = lm_fit)

by_subj <- ggplot(df, aes(x=factor(id), y=resid)) + 
  stat_summary(fun.data = mean_cl_boot, geom = "pointrange") + 
  geom_hline(yintercept=0, color="blue", size=2) +
  ggtitle("95% CIs on residuals by subject") +
  labs(x=NULL) + ylim(-1.5, 1.6) +
  theme_cowplot() +
  theme(axis.text.x = element_blank())

overall <- ggplot(df, aes(x=1, y=resid)) + geom_boxplot() + 
  # stat_summary(fun.data = mean_cl_boot, geom = "pointrange") +
  geom_hline(yintercept=0, color="blue", size=2) +
  ggtitle("Overall") +
  labs(x=NULL) + ylim(-1.5, 1.6) +
  theme_cowplot() +
  theme(axis.text.x = element_blank())

cowplot::plot_grid(by_subj, overall, nrow=1,rel_widths = c(0.8, 0.2))
```

By eye, it looks a bit suspicious, right?! Let's re-use the GLM to look at problems with the GLM. Here's a model that
treats id as a factor and looks at whether there is significant variation across ids in the residuals.

```{r}
mm <- lm(resid ~ factor(id), df)
summary(mm)
car::Anova(mm, type=3)
```

So, yeah, an F-test of almost 100 is probably worth attention.

More formally, we know that the ICC can give us a useful guide on whether clustering within units is substantial and, therefore, problematic for the GLM. Here, we calculate ICCs from the residuals of the GLM (i.e., removing variance due to condition and block).

```{r}
rfx_model <- lmer(resid ~ 1 + (1|id), df)
performance::icc(rfx_model)
```

*Bonus* for reflection: is it a coincidence that this ICC is identical to the $R^2$ from the GLM of residuals with subject as a factor? No, it's not -- take some time to think on why these are equivalent.

# Alternative: model each subject

Conceptually, when we have repeated measures data, we are often interested in the possibility that certain relationships vary from one person to the next. Indeed, these between-person differences might be related to other
individual difference variables such as personality traits.

What if we wanted to take this approach here and get a sense of the how experimental effects differ by subject?

## Visualize RT effects by subject

Here are boxplots for the first 25 subjects
```{r bigplot, fig=TRUE}
ggplot(flanker %>% filter(id <= 25), aes(x=cond, y=rt, color=block)) + 
  geom_boxplot() +
  facet_wrap(~id)
```

Definitely some heterogeneity there!

## Looking at coefficients by subject

What if we truly treated data for each subject separately? We could split up the data by subject 
and run a model with each person as a separate dataset.

```{r coef_map, fig=TRUE, fig.height=8, fig.width=5}
#a bit beyond the scope, but at a high level, we use dplyr and broom to run the same RT
#Documentation: https://cran.r-project.org/web/packages/broom/vignettes/broom_and_dplyr.html
res <- flanker %>%
  group_by(id) %>%
  nest() %>%
  mutate(test=map(data, ~ lm(rt_inv ~ block*cond, .x, na.action = na.exclude)),
         stats=map(test, tidy)) %>%
  unnest(stats) %>% dplyr::select(-data, -test) %>%
  ungroup() %>% group_by(term) %>% 
  mutate(estimate_termz=as.vector(scale(estimate))) %>% 
  ungroup()

ggplot(res, aes(x=term, y=id, fill=estimate_termz)) + 
  geom_tile() + scale_fill_viridis_c() + ggtitle("Coefficients") +
  theme(axis.text.x=element_text(angle=90))
```

```{r coef_boxplot, fig=TRUE, fig.height=6, fig.width=5}
overall <- data.frame(term=names(coef(lm_fit)), estimate=coef(lm_fit))
ggplot(res, aes(x=1, y=estimate)) + geom_boxplot() + facet_wrap(~term, scales="free") + 
  geom_point(data=overall, size=6, color="blue", shape=18) + 
  labs(title="Heterogeneity in within-person model coefficients",
       subtitle="Blue diamonds are coefs from overall lm()", x=NULL) +
  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
```

# Overview of mixed effects in lmer 

Let's move into the formal MLM framework to examine these data. Here is the basic model for a random intercept model, where we allow the intercept to vary by cluster, but keep the condition effects (`cond` and `block`) homogeneous (i.e., fixed only).

$$
\textrm{RT}_{ij} = \beta_{0j} + \beta_{1} \textrm{block}_{ij} + \beta_{2} \textrm{cond}_{ij} +
  \beta_{3}  \textrm{block}_{ij} \cdot \textrm{cond}_{ij} + \varepsilon_{ij} \\
  \beta_{0j} = \gamma_{00} + \mu_{0j} \\
  \boldsymbol{\mu_{0.}} \sim \mathcal{N}(0, \tau_{00}) \\
  \boldsymbol{\varepsilon_{..}} \sim \mathcal{N}(0, \sigma^2)
$$

## Basic unconditional random intercept model 

(Also called unconditional means model)

MLM usually begins by running an unconditional model that allows for cluster variation in the mean/expectation/intercept. The term 'unconditional' refers to the fact that the outcome (`rt_inv`) is not conditioned on (regressed on) any predictor. In this scenario, the intercept of the model is nearly the same as the overall average in the sample (and the reason for small divergence would be a distraction at the moment).

Why is this model useful? It gives us a starting point in terms of the amount of variation in the outcome that is attributable to between-cluster (here, between persons) versus within-cluster variation.

```{r}
m1 <- lmer(rt_inv ~ 1 + (1|id), flanker)
summary(m1)

mean(flanker$rt_inv, na.rm=TRUE)

#store variance components
v1 <- as.data.frame(get_variance(m1)) %>% mutate(model="unconditional random intercept") %>%
  dplyr::select(model, var.fixed, var.intercept, var.residual)
```

As a quick overview of `lmer` syntax, all terms on the right-hand side that are not in parentheses are fixed effects in the model -- that is, the estimates reflect the sample average effect. Here, `~ 1` indicates that an overall intercept ($\gamma_{00}$) should be included in the model. Note that this the default, but it's useful to include it clarity.

All terms included inside parentheses are entered as random effects, introducing a variance component for each term. Here, `1` within the parentheses requests that the intercept varies across clusters. The `|` operator denotes the separation between variance components and clustering variables. Here, `id` is the clustering variable, telling `lmer` that all terms on the left of the 'pipe' vary across units of the clustering variable on the right.

To map this output onto our model:

1. $\gamma_{00} = -2.6323$ (estimate of `Intercept` under fixed effects)
2. $\tau_{00} = .0918$ (estimate of `Intercept` variation by `id` under random effects)
3. $\textrm{var}(\boldsymbol{\varepsilon_{..}}) = .1507$ (estimate of residual variability under random effects)

And remember our formula for ICC -- proportion of between-cluster variation relative to overall variation. With this in mind, we have the values to calculate it by hand:

```{r}
vc <- VarCorr(m1)
print(vc,comp=c("Variance"))
dd <- as.data.frame(vc)

ICC_by_hand <- dd$vcov[dd$grp=="id"] / (dd$vcov[dd$grp=="id"] + dd$vcov[dd$grp=="Residual"])
print(ICC_by_hand)

#helper function
performance::icc(m1)
```

### Important principle: good predictors reduce residual variation

This principle is intuitive, but often overlooked. When we fit a standard GLM, we hope that as we add good predictors into the model, $SS_\textrm{Error}$ goes down, while the $SS_\textrm{Model}$ goes up.

$$
\begin{align}
SS_\textrm{Total} &= \sum_{i=1}^{N}({y_i - \bar{y}})^2 \\
SS_\textrm{Model} &= \sum_{i=1}^{N}({\hat{y}_i - \bar{y}})^2 \\
SS_\textrm{Error} &= \sum_{i=1}^{N}({y_i - \hat{y}})^2 = SS_\textrm{Total} - SS_\textrm{Model}
\end{align}
$$

By extension, when we move into the multilevel framework, *each* variance component (not just $\sigma^2$) represents variation that could potentially be explained by one or more predictors.

In the MLM, however, we can think of each variance component as a 'compartment' of overall variance, where the compartments can vary by level (e.g., within- versus between-persons). For example, if we introduce a level 1 predictor, it can potentially explain variation -- and thus reduce the variance components -- for both the L1 residual variability ($\sigma^2$) and level 2 variability in intercepts ($\tau_{00}$). This relates to the discussion of parcelling within versus between variation in L1 predictors and the concept of a 'total effect' (that combines both L1 and L2 variation).

## Basic random intercepts model

Let's take a look at what happens to our model when we add in the design factors of `cond` and `block`. This is a basic random intercepts model where we have fixed effects of `cond` and `block` and intercepts that vary across subjects (random). A useful way to rewrite the MLM equation above is:

$$
\textrm{RT}_{ij} = \textrm{Intercept}_j + \beta_{1} \textrm{block}_{ij} + \beta_{2} \textrm{cond}_{ij} +
  \beta_{3}  \textrm{block}_{ij} \cdot \textrm{cond}_{ij} + \varepsilon_{ij} \\
  \textrm{Intercept}_{j} = \gamma_{00} + \mu_{0j} \\
  {\textbf{Intercepts}} \sim \mathcal{N}(\gamma_{00}, \tau_{00})
$$

This notation emphasizes that the intercepts are a *set* of values, one per subject, and are normally distributed with mean $\gamma_{00}$ and variance $\tau_{00}$.


```{r}
m2 <- lmer(rt_inv ~ block*cond + (1|id), flanker)
summary(m2)

v2 <- as.data.frame(get_variance(m2)) %>% mutate(model="random intercept cond+block") %>%
  dplyr::select(model, var.fixed, var.intercept, var.residual)
```

Let's look at the reduction in variance components with the addition of fixed effects for cond and block

```{r}
v1_long <-  v1 %>% pivot_longer(-model) %>% rename(value.unconditional = value) %>% dplyr::select(-model)
v2_long <-  v2 %>% pivot_longer(-model) %>% rename(value.addfixed = value) %>% dplyr::select(-model)
v1_long %>% left_join(v2_long) %>% mutate(re.change.pct=(1 - value.addfixed/value.unconditional)*100) %>%
  kable(digits=2)
```
Here, we see that introducing `cond*block` reduced L1 residual variability by about 2.6% but had no effect on variability in the intercepts.

*For discussion:* I mentioned above that adding L1 predictors -- which is what cond and block are -- has the potential to explain variation at both L1 and L2. Why didn't we see any reduction in the intercept variability here?

### Visualizing our random effects

We can obtain the estimated random effects for each cluster in an `lmer` model using `ranef`.

```{r}
res <- ranef(m2)$id
lattice::histogram(res$`(Intercept)`, xlab="Estimated subject intercepts (mu_0j)")
```

We can use `REsim` from `merTools` to get estimated random effects for each cluster (here, `id`), including uncertainties in these estimates. We can also plot these using `plotREsim`:

```{r}
#predictInterval(m2)   # for various model predictions, possibly with new data

simranef <- REsim(m2)  # mean, median and sd of the random effect estimates
simranef %>% head()
plotREsim(simranef)  # plot the interval estimates
```

This is a handy plotting function because it shows by eye the tendency for cluster-level deviations to differ significantly from zero. Recall that if all cluster deviations (here, $\mu_{0j}$) are approximately zero, we don't really need to model that variance component (random effect).

Furthermore, as discussed previously, if we add predictors (as fixed effects) into the model and they explain considerable between-subjects (or more generally, between-cluster) variability, the subject-specific intercepts will increasingly approach zero and the variance component $\tau_{00}$ will also diminish toward zero.

# Basic random slopes model

What if we allowed for the possibility that the effect of incongruent vs. congruent trials varied by subject? That is, we think that there is some overall effect of congruency, but also that this effect could be stronger or weaker in some people.

How does this modify our model above? To keep things simple for now (we'll come back to this), I'm falling back to an additive model that does not include the `cond*block` interaction.

$$
\begin{align}
\textrm{RT}_{ij} &= \beta_{0j} + \beta_{1} \textrm{block}_{ij} + \beta_{2j} \textrm{cond}_{ij} + \varepsilon_{ij} \\
  \beta_{0j} &= \gamma_{00} + \mu_{0j} \\
  \beta_{1j} &= \gamma_{10} \\
  \beta_{2j} &= \gamma_{20} + \mu_{2j} \\
  \boldsymbol{\varepsilon_{..}} &\sim \mathcal{N}(0, \sigma^2)
\end{align}
$$

$$
\begin{bmatrix} 
\mu_{0j} \\ 
\mu_{2j} 
\end{bmatrix} 
\sim \mathcal{N}(\begin{bmatrix} 0 \\ 0\end{bmatrix}, 
\begin{bmatrix} \tau_{00} & \tau_{\textrm{int,cond}} \\ 
\tau_{\textrm{int,cond}} & \tau_{11}\end{bmatrix})
$$
Here, $\tau_{\textrm{int,cond}}$ represents the covariance between the a subject's estimated intercept and their estimated slope for the cond-RT association.

```{r}
m3 <- lmer(rt_inv ~ block + cond + (1 + cond | id), flanker)
summary(m3)

print(unlist(get_variance(m3)))
simranef3 <- REsim(m3)  # mean, median and sd of the random effect estimates
plotREsim(simranef3) + facet_wrap(~term, scales="free", ncol=1) # plot the interval estimates
```

We can see by eye that a handful of subjects have congruency slopes that differ reliably from zero, though this is not particularly striking relative to the random intercept. 

## Visualizing these effects

```{r}
plot_model(m2, type = "pred", terms = c("cond", "block"), pred.type = "re") + 
  labs(x = "Condition", y = "RT inverse", title = "Predicted effects in model that excludes condition random slope")

#add back interaction to make it comparable to random intercept model (m2)
m4 <- lmer(rt_inv ~ block + cond + (1 + cond | id), flanker)

plot_model(m4, type = "pred", terms = c("cond", "block"), pred.type = "re") + 
  labs(x = "Condition", y = "RT inverse", title = "Predicted effects in model that includes condition random slope")

plot_model(m4, type = "re", terms = c("cond", "block")) + 
  #facet_wrap(~facet, scales="free_y") +
  labs(y = "Model effect", x = "ID", title = "Model that includes random slopes")

```

## What about approximate normality of random effects?

```{r}
rr <- ranef(m3)$id %>% pivot_longer(cols=everything())

ggplot(rr, aes(x=value)) + geom_histogram(bins=15) + 
  facet_wrap(~name, scales="free")
```

Not too bad. One of our assumptions in MLM is that the random effects are distributed as (approximately) multivariate normal. Here, I've plotted the univariate distributions. We would also perhaps want to check that the bivariate distribution is approximately normal.

If you really want to get fancy, you could formally test for multivariate normality using the `MVN` package, though this is overkill for most applications.

```{r}
ggplot(ranef(m3)$id, aes(x=`(Intercept)`, y=condincongruent)) + geom_density_2d()
```
